= Laboratorio

[#uso-basico]
== 1. Interactuar con un modelo

=== Preparación del entorno

Comenzamos en el #terminal azul#. Vamos a establecer un entorno virtual de Python para usar el CLI de InstructLab ya instalado.

[.console-input]
[source,bash]
----
cd ~/ilab
source venv/bin/activate
----

¡Perfecto! Ya podemos usar InstructLab con el comando `ilab`. Para empezar a trabajar, InstructLab necesita *inicializar* para trabajar con los modelos. El comando `ilab config init` se encarga de:

* Localizar la taxonomía por defecto (estructura de conocimientos y habilidades).
* Crear el archivo de configuración (config.yaml).

El archivo *config.yaml* contiene los valores predeterminados que nos permiten ajustar el comportamiento del modelo a nuestro gusto, como el número de CPUs.

[.console-input]
[source,bash]
----
ilab config init
----

[source,bash]
----
Path to taxonomy repo [taxonomy]:
-> Pulsa la tecla ENTER
----

[source,bash]
----
'taxonomy' seems to not exist or is empty. Should I clone?
-> Pulsa la tecla Y, después ENTER
----

El resultado final de ejecutar el comando es:

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----

=== Servir modelo

Con el entorno ya configurado, podemos descargar un modelo al directorio local y servirlo para interactuar con él. Por cuestión de tiempo, el modelo ya está descargado.

[.console-input]
[source,bash]
----
mkdir -p ~/ilab/models
cp ~/files/granite-7b-lab-Q4_K_M.gguf ~/ilab/models
cp ~/files/merlinite-7b-lab-Q4_K_M.gguf ~/ilab/models
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----
Espera a que el output del terminal azul muestre el siguiente mensaje:

[source,bash]
----
INFO ... After application startup complete see http://127.0.0.1:8000/docs for API.
----

¡Genial! Estamos listos para probar el LLM.

=== Chatear con el modelo

Vamos a dejar el modelo sirviéndose en el terminal donde hemos trabajado y usaremos el #terminal blanco#. Volvemos a activar el entorno virtual de Python, pero esta vez iniciamos una sesión de chat con el comando `ilab model chat`

[.console-input]
[source,bash]
----
cd ~/ilab
source venv/bin/activate
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----

En tu terminal debería aparecer:

[source,bash]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Prueba a escribir:

[.console-input]
[source,bash]
----
Can you give me a short summary of what Openshift is?
----

¡Genial! El modelo debería responder que Openshift es una plataforma de conterización desarrollada por Red Hat. 

Ahora, prueba a escribir en el chat: 

[.console-input]
[source,bash]
----
Can you give me a short summary of what InstructLab is?
----

Vaya. El modelo responde que InstructLab es una plataforma educativa, algo que no es cierto y se aleja de la realidad. Este error se suele denominar «*alucinación*» en el mundo de la IA. Para solucionarlo, toca pasar por el entrenamiento. ¡Manos a la obra!

Antes de continuar, vamos a cerrar la sesión de chat con el modelo que hemos iniciado en el #terminal blanco#. Escribe 'exit'. 

[.console-input]
[source,bash]
----
exit
----

[#entrenamiento]
== 2. Entrenamiento del Modelo

Una vez que conocemos un poco acerca de IntructLab, vamos a usar su potencial, centrándonos en *mejorar la taxonomía*. Añadiremos conocimiento sobre InstructLab al modelo para que sepa más del proyecto y pueda responder a nuestras preguntas. 

=== Entender la taxonomía

¿Te has preguntado por qué InstructLab se llama así?

El *método LAB* (**L**arge-scale **A**lignment for chat**B**ots) se basa en taxonomías.
Las taxonomías son archivos YAML que contienen conocimientos y habilidades que InstructLab usa para su generación de datos.

Echémosle un ojo a la taxonomía actual.

[source,bash]
----
taxonomy/
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

Vemos que la taxonomía incluye conocimiento sobre artes, ingeniería, geografía... Vamos a crear un directorio en el que insertar el conocimiento sobre InstructLab. Aún en el #terminal blanco#, introducimos el siguiente comando:

[.console-input]
[source,bash]
----
mkdir -p ~/ilab/taxonomy/knowledge/instructlab/overview
----

En el entorno, ya hay preparado un archivo *qna.yaml*. InstructLab usa estos archivos para enseñar a los modelos. Estos contienen preguntas y respuestas sobre algo en concreto. Aquí tienes un ejemplo:

[source,bash]
----
- answer: The mission of instructlab is to let everyone shape generative AI
    by enabling contributed updates to existing LLMs in an accessible way.
    The community welcomes all those who would like to help enable everyone
    to shape the future of generative AI.
  question: 'What is the mission of Instructlab?'
----

Copiamos el archivo *qna.yaml* preparado en el directorio que hemos creado.

[.console-input]
[source,bash]
----
cp ~/files/qna.yml ~/ilab/taxonomy/knowledge/instructlab/overview
----

Para comprobar que hemos modificado correctamente la taxonomía, escribe el siguiente comando:

[.console-input]
[source,bash]
----
ilab taxonomy diff
----

Deberías obtener lo siguiente:

[source,bash]
----
Taxonomy in /taxonomy/ is valid :)
----

¡Perfecto! Tenemos todo listo para entrenar.

=== Entrenar modelo

Entrenar lleva *varias horas* y por cuestión de tiempo, tenemos preparado un modelo ya entrenado. ¡Como si fuera un programa de cocina!

En el entrenamiento, un modelo maestro (Merlinite en este caso) usa la taxonomía que hemos definido para generar más ejemplos de preguntas y respuestas. Después, entrenaremos al modelo con ellos. Cuantas más preguntas y respuestas, más sólido será el entrenamiento. El resultado será un nuevo modelo que comprenda el conocimiento que le hemos indicado.

[#interaccion]
== 3. Comprobar modelo entrenado

¡Hora de probar el modelo entrenado! Vamos al #terminal azul# y dejamos de servir el modelo antiguo usando `CTRL`+`C`. 

[source,bash]
----
INFO 2024-05-06 18:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

Luego, servimos el modelo preentrenado:

[.console-input]
[source,bash]
----
cp ~/files/ggml-ilab-pretrained-Q4_K_M.gguf ~/ilab/models
ilab model serve --model-path models/ggml-ilab-pretrained-Q4_K_M.gguf
----

Volvemos al #terminal blanco# e iniciamos el chat con el LLM.

[.console-input]
[source,bash]
----
ilab model chat --greedy-mode -m models/ggml-ilab-pretrained-Q4_K_M.gguf
----

¡Llega la hora de la verdad! Prueba a preguntar al LLM sobre InstructLab:

[.console-input]
[source,bash]
----
Can you give me a short summary of what InstructLab is?
----

¡Yuju! La respuesta debería ser mucho mejor que la última vez. El LLM debe ser capaz de explicar que InstructLab.

== Conclusión

*¡Laboratorio terminado con éxito!* Esperamos que hayas disfrutado probando de primera mano el potencial de InstructLab. Como pequeño repaso, has conseguido lo siguiente:

* Chatear con un LLM
* Modificar la taxonomía de InstructLab
* Comprobar el desempeño del modelo entrenado

Gracias por haber dedicado tu esfuerzo y tiempo en a aprender más sobre inteligencia articial y LLMs. Para más información sobre InstructLab, ¡echa un ojo a la comunidad en Github! https://github.com/instructlab


